---
title: "Predicting Exercise Activity"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 


## Loading data

```{r results = 'hide', message = FALSE}
library(caret)
library(doParallel)
if (!file.exists('train.csv')) {
  download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', 
                destfile = 'train.csv', method = 'curl') 
}
if (!file.exists('test.csv')) {
  download.file(url = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', 
                destfile = 'test.csv', method = 'curl')
}
trainData <- read.csv('train.csv')
testData <- read.csv('test.csv')
```

## Exploring and cleaning data

```{r results = 'hide', message = FALSE}
names(trainData)
names(testData)
```
There's no "classe" column in test data so we'll split the training data
```{r}
set.seed(10)
inTrain <- createDataPartition(y = trainData$classe, p = 0.7, list = F)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
```

Deleting irrelevant columns
```{r}
training <- training[,-1:-7]
testing <- testing[,-1:-7]
```

Deleting columns filled with NAs
```{r}
training <- training[, (colSums(is.na(training)) == 0)]
testing <- testing[, (colSums(is.na(testing)) == 0)]
```

Removing zero covariates
```{r}
nzv <- nearZeroVar(training, saveMetrics = T)
training <- training[, row.names(nzv[nzv$nzv == FALSE, ])]
testing <- testing[, row.names(nzv[nzv$nzv == FALSE, ])]
training$classe <- as.factor(training$classe)
testing$classe <- as.factor(testing$classe)
```

## Modeling

```{r echo=FALSE}
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
```

Setup cross-validation. It was found out that 3 subsamples is good for accuracy
and speed.
```{r}
fitControl <- trainControl(method = "cv", number = 3, allowParallel = TRUE)
```

We'll make 3 models and compare their accuracy and speed:

Model 1: Applying Linear Discriminant Analysis (LDA)
```{r cache = TRUE}
set.seed(10)
ldaModel <- train(classe ~. , data = training, method = 'lda')
```

Predict test outcomes using this model
```{r}
pred.lda <- predict(ldaModel, newdata = testing)
confusionMatrix(pred.lda, testing$classe)$table
confusionMatrix(pred.lda, testing$classe)$overall[1]
```
The accuracy for LDA model is 70.7% 

Model 2: Applying Random Forests
```{r cache = TRUE}
rfModel <- train(classe ~. , data = training, method = 'rf',trControl = fitControl)
```

Predict test outcomes using this model
```{r}
pred.rf <- predict(rfModel, newdata = testing)
confusionMatrix(pred.rf, testing$classe)$table
confusionMatrix(pred.rf, testing$classe)$overall[1]
```
The accuracy for random forest model is 99.2% 

Model 3: Applying Gradient Boosting Method
```{r cache = TRUE}
gbmModel <- train(classe ~. , data = training, method = 'gbm',trControl = fitControl)
```

Predict test outcomes using this model
```{r}
pred.gbm <- predict(gbmModel, newdata = testing)
confusionMatrix(pred.gbm, testing$classe)$table
confusionMatrix(pred.gbm, testing$classe)$overall[1]
```
The accuracy for gradient boosting method model is 95.4% 

## Conclusions
It was expected that random forest and boosting would be the most accurate and 
so it was. Random tree forest was the slowest of all three and LDA was the less
accurate.


